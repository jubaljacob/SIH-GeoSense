{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting googlemaps\n",
      "  Downloading googlemaps-4.10.0.tar.gz (33 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests<3.0,>=2.20.0 in c:\\users\\jubal\\appdata\\roaming\\python\\python311\\site-packages (from googlemaps) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\jubal\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jubal\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\jubal\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jubal\\appdata\\roaming\\python\\python311\\site-packages (from requests<3.0,>=2.20.0->googlemaps) (2023.5.7)\n",
      "Installing collected packages: googlemaps\n",
      "  Running setup.py install for googlemaps: started\n",
      "  Running setup.py install for googlemaps: finished with status 'done'\n",
      "Successfully installed googlemaps-4.10.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: googlemaps is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\n",
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install -U googlemaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: north, No matching canonical name found\n",
      "Token: korea, No matching canonical name found\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "model_dir = 'DistilBert_conll03'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_dir)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"Jordan is fram from north-korea\"\n",
    "\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "def aggregate_subwords(current_entity, final_entities):\n",
    "    if current_entity:\n",
    "        entity_word = ''.join([word.replace('##', '') for word in current_entity[\"words\"]])\n",
    "        final_entities.append(entity_word)\n",
    "\n",
    "current_entity = {}\n",
    "final_entities = []\n",
    "for entity in ner_results:\n",
    "    if entity['entity'] in ['B-LOC', 'I-LOC']:\n",
    "        word = entity['word'].replace('##', '')\n",
    "        if entity['word'].startswith('##') or entity['word'].endswith('-') or (current_entity.get(\"words\") and current_entity[\"words\"][-1].endswith('-')):\n",
    "            current_entity[\"words\"].append(word)\n",
    "        else:\n",
    "            aggregate_subwords(current_entity, final_entities)\n",
    "            current_entity = {\"words\": [word]}\n",
    "\n",
    "aggregate_subwords(current_entity, final_entities)\n",
    "\n",
    "similarity_threshold = 80\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            row['other-names'] = [name.strip() for name in row['other-names'].split(',')]\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "def perform_fuzzy_matching(extracted_name, dataset):\n",
    "    best_match = None\n",
    "    highest_similarity = 0\n",
    "    for row in dataset:\n",
    "\n",
    "        similarity = fuzz.ratio(extracted_name.lower(), row[\"canonical name\"].lower())\n",
    "        if similarity > similarity_threshold and similarity > highest_similarity:\n",
    "            best_match = row\n",
    "            highest_similarity = similarity\n",
    "            \n",
    "        for other_name in row['other-names']:\n",
    "            if other_name:\n",
    "                similarity = fuzz.ratio(extracted_name.lower(), other_name.lower())\n",
    "                if similarity > similarity_threshold and similarity > highest_similarity:\n",
    "                    best_match = row\n",
    "                    highest_similarity = similarity\n",
    "    return best_match\n",
    "\n",
    "dataset = load_dataset('Datasets/place_name.csv')\n",
    "\n",
    "for extracted_name in final_entities:\n",
    "    best_match = perform_fuzzy_matching(extracted_name, dataset)\n",
    "    if best_match:\n",
    "        canonical_name = best_match[\"canonical name\"]\n",
    "        place_type = best_match[\"place-type\"]\n",
    "        other_names = ', '.join(best_match[\"other-names\"])\n",
    "        print(f\"Token: {extracted_name}, Canonical name: {canonical_name}, Place Type: {place_type}\")\n",
    "    else:\n",
    "        print(f\"Token: {extracted_name}, No matching canonical name found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid API key provided.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\CODE\\SIH\\GeoSense-SIH\\Combined.ipynb Cell 3\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CODE/SIH/GeoSense-SIH/Combined.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m google_maps_api_key \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mYOUR_API_KEY_HERE\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/CODE/SIH/GeoSense-SIH/Combined.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Initialize the Google Maps client\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/CODE/SIH/GeoSense-SIH/Combined.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m gmaps \u001b[39m=\u001b[39m googlemaps\u001b[39m.\u001b[39;49mClient(key\u001b[39m=\u001b[39;49mgoogle_maps_api_key)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE/SIH/GeoSense-SIH/Combined.ipynb#W6sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_geolocation\u001b[39m(place_name):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE/SIH/GeoSense-SIH/Combined.ipynb#W6sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/CODE/SIH/GeoSense-SIH/Combined.ipynb#W6sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m         \u001b[39m# Use the Geocoding API to get the geographic coordinates\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\googlemaps\\client.py:144\u001b[0m, in \u001b[0;36mClient.__init__\u001b[1;34m(self, key, client_id, client_secret, timeout, connect_timeout, read_timeout, retry_timeout, requests_kwargs, queries_per_second, queries_per_minute, channel, retry_over_query_limit, experience_id, requests_session, base_url)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mMust provide API key or enterprise credentials \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    141\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mwhen creating client.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    143\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m key\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mAIza\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 144\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mInvalid API key provided.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    146\u001b[0m \u001b[39mif\u001b[39;00m channel:\n\u001b[0;32m    147\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m re\u001b[39m.\u001b[39mmatch(\u001b[39m\"\u001b[39m\u001b[39m^[a-zA-Z0-9._-]*$\u001b[39m\u001b[39m\"\u001b[39m, channel):\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid API key provided."
     ]
    }
   ],
   "source": [
    "import googlemaps\n",
    "from datetime import datetime\n",
    "\n",
    "# Your Google Maps API key\n",
    "google_maps_api_key = \"YOUR_API_KEY_HERE\"\n",
    "\n",
    "# Initialize the Google Maps client\n",
    "gmaps = googlemaps.Client(key=google_maps_api_key)\n",
    "\n",
    "def get_geolocation(place_name):\n",
    "    try:\n",
    "        # Use the Geocoding API to get the geographic coordinates\n",
    "        geocode_result = gmaps.geocode(place_name)\n",
    "\n",
    "        if geocode_result:\n",
    "            location = geocode_result[0][\"geometry\"][\"location\"]\n",
    "            return location[\"lat\"], location[\"lng\"]\n",
    "        else:\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"Error while fetching geolocation:\", e)\n",
    "        return None\n",
    "\n",
    "for extracted_name in final_entities:\n",
    "    best_match = perform_fuzzy_matching(extracted_name, dataset)\n",
    "    if best_match:\n",
    "        canonical_name = best_match[\"canonical name\"]\n",
    "        place_type = best_match[\"place-type\"]\n",
    "        other_names = ', '.join(best_match[\"other-names\"])\n",
    "        print(f\"Token: {extracted_name}, Canonical name: {canonical_name}, Place Type: {place_type}\")\n",
    "\n",
    "        # Get geolocation for the canonical name\n",
    "        coordinates = get_geolocation(canonical_name)\n",
    "\n",
    "        if coordinates:\n",
    "            latitude, longitude = coordinates\n",
    "            print(f\"Coordinates (Latitude, Longitude): {latitude}, {longitude}\")\n",
    "\n",
    "            # Now, you can use the latitude and longitude to display the location on Google Maps\n",
    "            # You can open a web browser or use a web-based mapping library (e.g., Folium) to display the location.\n",
    "            # Here's an example using the webbrowser library:\n",
    "\n",
    "            import webbrowser\n",
    "\n",
    "            map_url = f\"https://www.google.com/maps/place/{latitude},{longitude}\"\n",
    "            webbrowser.open(map_url)\n",
    "\n",
    "    else:\n",
    "        print(f\"Token: {extracted_name}, No matching canonical name found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: sydney, No matching canonical name found\n",
      "Token: tamilnadu, Canonical name: Tamil Nadu, Place Type: State\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "from transformers import pipeline\n",
    "import csv\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "model_dir = 'DistilBert_conll03'\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_dir)\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"Sydney is going to tamil nadu\"\n",
    "\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "def aggregate_subwords(current_entity, final_entities):\n",
    "    if current_entity:\n",
    "        entity_word = ''.join([word.replace('##', '') for word in current_entity[\"words\"]])\n",
    "        if not final_entities or (final_entities and final_entities[-1] != entity_word):\n",
    "            final_entities.append(entity_word)\n",
    "\n",
    "current_entity = {}\n",
    "final_entities = []\n",
    "previous_entity = None\n",
    "\n",
    "for entity in ner_results:\n",
    "    if entity['entity'] in ['B-LOC', 'I-LOC']:\n",
    "        word = entity['word'].replace('##', '')\n",
    "        if entity['word'].startswith('##') or entity['word'].endswith('-') or (current_entity.get(\"words\") and current_entity[\"words\"][-1].endswith('-')):\n",
    "            current_entity[\"words\"].append(word)\n",
    "        elif previous_entity == 'B-LOC' and entity['entity'] == 'I-LOC':\n",
    "            current_entity[\"words\"].append(word)\n",
    "        else:\n",
    "            aggregate_subwords(current_entity, final_entities)\n",
    "            current_entity = {\"words\": [word]}\n",
    "        previous_entity = entity['entity']\n",
    "    else:\n",
    "        previous_entity = None\n",
    "\n",
    "aggregate_subwords(current_entity, final_entities)\n",
    "\n",
    "similarity_threshold = 80\n",
    "\n",
    "def load_dataset(file_path):\n",
    "    dataset = []\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.DictReader(csvfile)\n",
    "        for row in reader:\n",
    "            row['other-names'] = [name.strip() for name in row['other-names'].split(',')]\n",
    "            dataset.append(row)\n",
    "    return dataset\n",
    "\n",
    "def perform_fuzzy_matching(extracted_name, dataset):\n",
    "    best_match = None\n",
    "    highest_similarity = 0\n",
    "    for row in dataset:\n",
    "\n",
    "        similarity = fuzz.ratio(extracted_name.lower(), row[\"canonical name\"].lower())\n",
    "        if similarity > similarity_threshold and similarity > highest_similarity:\n",
    "            best_match = row\n",
    "            highest_similarity = similarity\n",
    "            \n",
    "        for other_name in row['other-names']:\n",
    "            if other_name:\n",
    "                similarity = fuzz.ratio(extracted_name.lower(), other_name.lower())\n",
    "                if similarity > similarity_threshold and similarity > highest_similarity:\n",
    "                    best_match = row\n",
    "                    highest_similarity = similarity\n",
    "    return best_match\n",
    "\n",
    "dataset = load_dataset('Datasets/place_name.csv')\n",
    "\n",
    "for extracted_name in final_entities:\n",
    "    best_match = perform_fuzzy_matching(extracted_name, dataset)\n",
    "    if best_match:\n",
    "        canonical_name = best_match[\"canonical name\"]\n",
    "        place_type = best_match[\"place-type\"]\n",
    "        other_names = ', '.join(best_match[\"other-names\"])\n",
    "        print(f\"Token: {extracted_name}, Canonical name: {canonical_name}, Place Type: {place_type}\")\n",
    "    else:\n",
    "        print(f\"Token: {extracted_name}, No matching canonical name found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: maharashtra, Canonical name: Maharashtra, Place Type: state\n",
      "Token: ahmedabad, Canonical name: Ahmedabad, Place Type: city\n",
      "Token: new, No matching canonical name found\n",
      "Token: -, No matching canonical name found\n",
      "Token: zealand, Canonical name: newzealand, Place Type: country\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: jordan, No matching canonical name found\n",
      "Token: tamilnadu, Canonical name: Tamil Nadu, Place Type: State\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "# from transformers import pipeline\n",
    "# import csv\n",
    "# from fuzzywuzzy import fuzz\n",
    "\n",
    "# model_dir = 'DistilBert_conll03'\n",
    "# tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "# model = DistilBertForTokenClassification.from_pretrained(model_dir)\n",
    "# ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# text = \"Jordan is going to Jordan, his native is tamil nadu\"\n",
    "\n",
    "# ner_results = ner_pipeline(text)\n",
    "\n",
    "# def aggregate_subwords(current_entity, final_entities):\n",
    "#     if current_entity:\n",
    "#         entity_word = ''.join([word.replace('##', '') for word in current_entity[\"words\"]])\n",
    "#         if not final_entities or (final_entities and final_entities[-1] != entity_word):\n",
    "#             final_entities.append(entity_word)\n",
    "\n",
    "# current_entity = {}\n",
    "# final_entities = []\n",
    "# previous_entity = None\n",
    "\n",
    "# for entity in ner_results:\n",
    "#     if entity['entity'] in ['B-LOC', 'I-LOC']:\n",
    "#         word = entity['word'].replace('##', '')\n",
    "#         if entity['word'].startswith('##') or entity['word'].endswith('-') or (current_entity.get(\"words\") and current_entity[\"words\"][-1].endswith('-')):\n",
    "#             current_entity[\"words\"].append(word)\n",
    "#         elif previous_entity == 'B-LOC' and entity['entity'] == 'I-LOC':\n",
    "#             current_entity[\"words\"].append(word)\n",
    "#         else:\n",
    "#             aggregate_subwords(current_entity, final_entities)\n",
    "#             current_entity = {\"words\": [word]}\n",
    "#         previous_entity = entity['entity']\n",
    "#     else:\n",
    "#         previous_entity = None\n",
    "\n",
    "# aggregate_subwords(current_entity, final_entities)\n",
    "\n",
    "# similarity_threshold = 80\n",
    "\n",
    "# def load_dataset(file_path):\n",
    "#     dataset = []\n",
    "#     with open(file_path, 'r', newline='', encoding='utf-8') as csvfile:\n",
    "#         reader = csv.DictReader(csvfile)\n",
    "#         for row in reader:\n",
    "#             row['other-names'] = [name.strip() for name in row['other-names'].split(',')]\n",
    "#             dataset.append(row)\n",
    "#     return dataset\n",
    "\n",
    "# def perform_fuzzy_matching(extracted_name, dataset):\n",
    "#     best_match = None\n",
    "#     highest_similarity = 0\n",
    "#     for row in dataset:\n",
    "\n",
    "#         similarity = fuzz.ratio(extracted_name.lower(), row[\"canonical name\"].lower())\n",
    "#         if similarity > similarity_threshold and similarity > highest_similarity:\n",
    "#             best_match = row\n",
    "#             highest_similarity = similarity\n",
    "            \n",
    "#         for other_name in row['other-names']:\n",
    "#             if other_name:\n",
    "#                 similarity = fuzz.ratio(extracted_name.lower(), other_name.lower())\n",
    "#                 if similarity > similarity_threshold and similarity > highest_similarity:\n",
    "#                     best_match = row\n",
    "#                     highest_similarity = similarity\n",
    "#     return best_match\n",
    "\n",
    "# dataset = load_dataset('Datasets/place_name.csv')\n",
    "\n",
    "# for extracted_name in final_entities:\n",
    "#     best_match = perform_fuzzy_matching(extracted_name, dataset)\n",
    "#     if best_match:\n",
    "#         canonical_name = best_match[\"canonical name\"]\n",
    "#         place_type = best_match[\"place-type\"]\n",
    "#         other_names = ', '.join(best_match[\"other-names\"])\n",
    "#         print(f\"Token: {extracted_name}, Canonical name: {canonical_name}, Place Type: {place_type}\")\n",
    "#     else:\n",
    "#         print(f\"Token: {extracted_name}, No matching canonical name found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
