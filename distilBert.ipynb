{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: tamil, Label: B-LOC\n",
      "Token: nadu, Label: I-LOC\n",
      "Token: korea, Label: B-LOC\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "from transformers import pipeline\n",
    "\n",
    "model_dir = 'DistilBert_conll03'\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n",
    "model = DistilBertForTokenClassification.from_pretrained(model_dir)\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"Tamil Nadu is far from north-korea\"\n",
    "ner_results = ner_pipeline(text)\n",
    "\n",
    "def aggregate_subwords(current_entity, final_entities):\n",
    "    if current_entity:\n",
    "        entity_word = ''.join(current_entity[\"words\"]).replace('##', '')\n",
    "        final_entities.append(f\"Token: {entity_word}, Label: {current_entity['label']}\")\n",
    "        # final_entities.append(f\"Token: {entity_word}\"  # , Label: {current_entity['label']}\n",
    "                              \n",
    "\n",
    "\n",
    "current_entity = {}\n",
    "final_entities = []\n",
    "\n",
    "for entity in ner_results:\n",
    "    if entity['word'].startswith('##'):\n",
    "        # This is a subword of the current entity\n",
    "        current_entity[\"words\"].append(entity['word'])\n",
    "    else:\n",
    "        aggregate_subwords(current_entity, final_entities)\n",
    "        current_entity = {\n",
    "            \"words\": [entity['word']],\n",
    "            \"label\": entity['entity']\n",
    "        }\n",
    "\n",
    "aggregate_subwords(current_entity, final_entities)\n",
    "\n",
    "# Print the final entities\n",
    "for entity in final_entities:\n",
    "    print(entity)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token: kovai\n",
    "CanonicalName: Coimbatore(kovai , blablabla)\n",
    "Table: State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import DistilBertTokenizer, DistilBertForTokenClassification\n",
    "# from fuzzywuzzy import process\n",
    "\n",
    "# # Load DistilBERT model for NER\n",
    "# # tokenizer = DistilBertTokenizer.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "# # model = DistilBertForTokenClassification.from_pretrained('dbmdz/bert-large-cased-finetuned-conll03-english')\n",
    "\n",
    "# distilbert-base-uncased\n",
    "\n",
    "# # Example tables\n",
    "# COUNTRIES = [\"newzealand\", \"india\", \"usa\"]\n",
    "# CITIES = [\"ahmedabad\", \"chennai\", \"newyork\"]\n",
    "# STATES = [\"maharashtra\", \"texas\", \"california\"]\n",
    "\n",
    "# # Function to extract place names using NER\n",
    "# def extract_place_names(sentence):\n",
    "#     tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(sentence)))\n",
    "#     inputs = tokenizer.encode(sentence, return_tensors=\"pt\")\n",
    "    \n",
    "#     outputs = model(inputs).logits\n",
    "#     predictions = torch.argmax(outputs, dim=2)\n",
    "    \n",
    "#     # The ids 4 and 5 correspond to location entities in the 'dbmdz/bert-large-cased-finetuned-conll03-english' model\n",
    "#     loc_tokens = [token for token, label in zip(tokens, predictions[0].tolist()) if label in [4, 5]]\n",
    "\n",
    "#     return loc_tokens\n",
    "\n",
    "# # Function to perform fuzzy matching and get canonical name\n",
    "# def fuzzy_match(name, table):\n",
    "#     matched_name, score = process.extractOne(name, table)\n",
    "#     if score > 80:  # You can adjust this threshold\n",
    "#         return matched_name\n",
    "#     return None\n",
    "\n",
    "# # Main function\n",
    "# def geospatial_query_system(sentence):\n",
    "#     place_names = extract_place_names(sentence)\n",
    "    \n",
    "#     results = []\n",
    "#     for name in place_names:\n",
    "#         canonical_name = None\n",
    "#         source_table = None\n",
    "        \n",
    "#         for table, table_name in [(COUNTRIES, \"Country\"), (CITIES, \"City\"), (STATES, \"State\")]:\n",
    "#             matched_name = fuzzy_match(name, table)\n",
    "#             if matched_name:\n",
    "#                 canonical_name = matched_name\n",
    "#                 source_table = table_name\n",
    "#                 break\n",
    "        \n",
    "#         if canonical_name:\n",
    "#             results.append({\n",
    "#                 \"Token\": name,\n",
    "#                 \"Canonical name\": canonical_name,\n",
    "#                 \"Table\": source_table\n",
    "#             })\n",
    "    \n",
    "#     return results\n",
    "\n",
    "# # Example\n",
    "# sentence = \"Which of the following saw highest average temperature in January, Maharashtra, Ahmedabad or entire New-Zealand?\"\n",
    "# print(geospatial_query_system(sentence))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
